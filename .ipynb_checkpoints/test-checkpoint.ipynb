{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_distance(a , b):\n",
    "    '''Метрика косинусного расстояния'''\n",
    "    try:\n",
    "        return a@b/((a@a)*(b@b))**0.5\n",
    "    except:\n",
    "        return -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "where_from = 'rbc'\n",
    "\n",
    "with open(f'./data/news_{where_from}.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f'vectors_news_{where_from}.json' not in os.listdir(os.getcwd() + '\\\\data\\\\vectors\\\\'):\n",
    "\n",
    "    for id in data:\n",
    "        data[id]['news_embedding'] = embed_bert_cls(data[id]['news_title'], model, tokenizer).tolist()\n",
    "        data[id].pop('news_link', None)\n",
    "        data[id].pop('news_text', None)\n",
    "\n",
    "    with open(f'./data/vectors/vectors_news_{where_from}.json', 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "else:\n",
    "\n",
    "    with open(f'./data/vectors/vectors_news_{where_from}.json', 'r') as f:\n",
    "        data_vectors = json.load(f)\n",
    "\n",
    "    news_hashes = list(data_vectors.keys())\n",
    "\n",
    "    for id in data:\n",
    "        if id not in news_hashes:\n",
    "            data_vectors[id]['news_embedding'] = embed_bert_cls(data[id]['news_title'], model, tokenizer).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/LaBSE-en-ru were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/LaBSE-en-ru\")\n",
    "model = AutoModel.from_pretrained(\"cointegrated/LaBSE-en-ru\")\n",
    "sentences = [\"Hello World\", \"Привет Мир\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LaBSE(text):\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, max_length=64, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    embeddings = model_output.pooler_output\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7168594693891301"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'Охранник отказался покидать свой пост'\n",
    "b = 'Джонсон не любит министров'\n",
    "\n",
    "cos_distance(LaBSE(a), LaBSE(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Phrases\n",
    "from razdel import tokenize, sentenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spm_encode(string):\n",
    "    '''Кодируем строку в BPE-Dropout вектор'''\n",
    "    return sp.encode(string, nbest_size=-1, out_type='str')\n",
    "    # return sp.encode(string, enable_sampling=True, alpha=0.1, nbest_size=-1, out_type='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent(text):\n",
    "    return [sent.text for sent in list(sentenize(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(string):\n",
    "    tokens = list(tokenize(string))\n",
    "    return [_.text for _ in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = ['rbc', 'lenta', 'ria']\n",
    "\n",
    "for where_from in sources:\n",
    "\n",
    "    with open(f'./data/news_{where_from}.json', 'r', encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if where_from == 'rbc':\n",
    "        all_titles = [data[i]['news_text'] for i in data]\n",
    "    else:\n",
    "        all_titles.extend([data[i]['news_text'] for i in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_titles_sent = []\n",
    " \n",
    "for text in all_titles:\n",
    "    all_titles_sent.extend(get_sent(text))\n",
    "\n",
    "all_titles_sent_tokens = [get_tokens(sent) for sent in all_titles_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'./data/sentencepeace/titles.txt', 'w', encoding=\"utf-8\") as f:\n",
    "#     f.write(\"\\n\".join(all_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spm.SentencePieceTrainer.train(input = './data/sentencepeace/titles.txt', model_prefix = 'm', vocab_size = 1000, model_type = 'bpe')\n",
    "# sp = spm.SentencePieceProcessor(model_file = 'm.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spm_encode_ids = [spm_encode(text) for text in all_titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 20.4 s\n",
      "Wall time: 6.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_wv = Word2Vec(sentences = all_titles_sent_tokens, vector_size=2000, window=5, min_count=1, workers=8)\n",
    "model_wv.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_cast(x):\n",
    "    c = 0\n",
    "    s = model_wv.wv[0] * 0\n",
    "    \n",
    "    for i in x:\n",
    "        if i in model_wv.wv:\n",
    "            if c != 0:\n",
    "                s = s + model_wv.wv[i]\n",
    "            else:\n",
    "                s = model_wv.wv[i]\n",
    "            c += 1\n",
    "        else:\n",
    "            continue\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9990397132164921"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'В Госдепе обещали усилить санкции против России'\n",
    "text_b = 'В США '\n",
    "cos_distance(w2v_cast(get_tokens(text)), w2v_cast(get_tokens(text_b)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce4fa7460189f37aad251fe24cbc112a94fefd6d641b35edeea050628a4afc4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
